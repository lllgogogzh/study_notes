[TOC]



# Linear-Gaussian Estimation

| Words         | Translate |
| ------------- | --------- |
| recursive     | 递归的    |
| deterministic | 决定性的  |
| sparse        | 稀疏的    |
| exploit       | 利用      |

## 3.1 Batch Discrete-Time Estimation

*译作：离散时间的批量估计*

### 3.1.1 Problem Setup

​	这章节的大部分内容，我们考虑**离散时间、线性、时变的方程**。我们定义如下的运动以及观测模型：
$$
motion  \ model :
x_k=A_{k-1}x_{k-1}+v_k+w_k, \ \ k=1...K\\
observation \  model:
y_k=C_kx_k+n_k, \ k=0 ... K
$$
其中，k是离散时间下标，最大值为K。上述符号意义如下：

![](img\19.png)

​	除了vk以外，这些都是随机变量。**噪声和初始状态我们均假设各不相关，而且不同时刻也不相关**。矩阵Ak称为**转移矩阵(transition matrix)**，矩阵Ck∈R^(M×N)被称为**观测矩阵(observation matrix)**。

#### 状态估计目的

​	我们的首要目的：估计(得到)系统的所有时间的所有状态。

​	我们的已知：

1. 初始状态x0h以及其协方差矩阵P0h；我们有时候不知道初始信息，就需要在不知道的情况下进行推导。
2. 输入vk，来自我们的控制器输出，已知，而且其噪声协方差矩阵Qk。
3. 测量值yk,meas，是yk的一次**实现**，噪声协方差：Rk

#### 状态估计问题

​	**状态估计问题是指：在k个(一个或多个)时间点上，基于初始的状态信息x0h、一系列观测数据y(0:K,meas)、一系列输入v(1:K)，以及系统的运动和观测模型，来计算系统的真实状态的估计值x(k)h。**



​	我们回到LG(Linear Gaussian)系统的估计问题：我们用两个不同的途径解决此问题：

1. **贝叶斯推断(Bayesian inference)：**我们从状态的先验概率密度函数出发，通过初始状态、输入、运动方程和观测数据，计算状态的后验概率密度函数。
2. **最大后验估计(Maximum A Posteriori MAP)**：我们用优化理论，来寻找给定信息下(初始状态、输入、观测)的最大后验估计。

### 3.1.2 Maximum A Posteriori

*译作：最大后验估计*

​	在批量估计问题中，我们的目标是解决如下MAP问题：
$$
\hat{x}=argmax_xp(x|v,y)
$$
我们希望在给定先验信息和所有时刻的运动v、观测y的情况下，推测出所有时刻的最优状态xhat。

​	为此，我们定义一些数据：**我们用上帽子表示后验估计(包含了观测量的)，用下帽子表示先验估计(不含观测的)**
$$
x=x_{0:K}=(x_0,...,x_K)\\
v=(\check x_0,v_1,...,v_K)\\
y=y_{0:K}=(y_0,...,y_K)
$$
**我们首先利用贝叶斯公式重写MAP：**过程详解
$$
\hat{x}=argmax_xp(x|v,y)=argmax_x\frac{p(y|x,v)p(x|v)}{p(y|v)}=argmax_xp(y|x)p(x|v)
$$

$$
p(x|v,y)=(贝叶斯公式)\frac{p(v,y|x)p(x)}{p(v,y)}
\\=(条件概率公式)\frac{p(v,y|x)p(x)}{p(y|v)p(v)}
\\=(条件概率公式)\frac{p(v,y,x)}{p(y|v)p(v)}
\\=(条件概率公式)\frac{p(y|x,v)p(x,v)}{p(y|v)\frac{p(x,v)}{p(x|v)}}
\\ (化简)=\frac{p(y|x,v)p(x|v)}{p(y|v)}
$$

最后一个等式的解释：分母p(y|v)与变量x无关，可以当作常数；分子中，p(y|x,v)的v可以省略，因为如果x已知，v不会影响观测数据(观测方程与其无关)。

​	接下来，**我们做出一个重要假设：噪声wk和nk之间无关**。这样我们有：
$$
p(y|x)=\prod_{k=0}^{K}{p(y_k|x_k)}(根据观测方程可以得到)
$$

另一方面，贝叶斯定理允许我们将p(x|v)分解为：
$$
p(x|v)=p(x_0|\check{x_0})\prod_{k=1}^Kp(x_k|x_{k-1},v_k)\\
(根据输入方程可得)
$$
线性系统中，高斯密度函数可展开为：
$$
p(x_0|\check{x_0})=\frac{1}{\sqrt{(2\pi)^Ndet(\check P_0)}}×exp(-\frac{1}{2}(x_0-\check{x_0})\check{P_0^{-1}}(x_0-\check{x_0}))
$$

$$
p(x_k|x_{k-1},v_k)=(根据运动方程)\frac{1}{\sqrt{}}
$$

$$
p(y_k|x_k)=(根据观测方程)\frac{1}{\sqrt{}}
$$

我们对优化的式子取对数(由于对数函数是**单调递增的(monotonically increasing)**，因此对我们的优化问题没有影响)。
$$
ln(p(y|x)p(x|v))=ln(p(x_0|\check{x_0}))+\sum_{k=1}^Kp(x_k|x_{k-1},v_k)+\sum_{k=0}^{K}p(y_k|x_k)
$$
将上式代入，并且拿掉与x无关的常数项，我们定义：
$$
J_{v,k}(x)=\begin{cases}
\frac{1}{2}(x_0-\check{x_0})^T\check{P}^{-1}_0(x_0-\check{x_0}) \ \ \ \ \ \ ,\ \ \ k=0\\
\frac{1}{2}(x_k-A_{k-1}x_{k-1}-v_k)^TQ_k^{-1}(x_k-A_{k-1}x_{k-1}-v_k)\ \ \ \ \ \ , \ \ \ k=1...K\\
\end{cases}\\
J_{y,k}(x)=\frac{1}{2}(y_k-C_kx_k)^TR_k^{-1}(y_k-C_kx_k)\ \ \ \ \ \ , \ \ \ k = 0...K
$$
**这些都是平方马氏距离(Mahalanobis distances)。**我们对整体定义**目标函数(objective function)**，寻找最小值
$$
J(x)=\sum_{k=0}^K{(J_{v,k}(x)+J_{y,k}(x))}
$$
我们还可以对Jx函数进行修改(添加限制或者惩罚项)，这些看情况而定。

​	综上所述，**我们把最大后验估计搞成了求Jx的最小值的问题。**也是无约束最优化问题。
$$
\hat{x}=argmin_xJ(x)
$$


##### 解此无约束最优化问题：

​	我们想办法把函数搞成x的平方的形式：

我们令所有已知量为z，所有待估计量为x：
$$
z=[\check{x_0},v_1,...,v_K,y_0,...,y_K]^T\\
x=[x_0,...,x_K]^T
$$
我们定义以下块矩阵：？？？？？？？？？？？？？

![](img\20.png)

我们可以得到：
$$
J(x)=\frac{1}{2}(z-Hx)^TW^{-1}(z-Hx)
$$
同时，我们也有：
$$
p(z|x)=ηexp(-\frac{1}{2}(z-Hx)^TW^{-1}(z-Hx))
$$
η为归一化常数。此时，我们对J(x)求导数：
$$
\frac{dJ(x)}{dx}=-H^TW^{-1}(z-H\hat{x})=0\\
(H^Tw^{-1}H)\hat{x}=H^TW^{-1}z
$$
不过这个求逆不需要硬求，可以利用矩阵的稀疏性，后续会讲解。

##### 形象化

![](img\21.png)

如图所示，批量估计问题的一个直观解释。函数的每一项可以认为是弹簧中储存的能量，根据运动方程可以得到先验，其中先验与上一个状态有关；而测量量直接与状态量相关，这样我们使系统能量最小即可得到后验估计。

### 3.1.3 贝叶斯推断

​	我们已经见到通过优化方法来进行批量线性高斯估计的方法，我们现在来通过贝叶斯法则，而不是最大化后验。**我们要从先验的概率密度出发，然后再用测量的数据来更新它。**

​	*单独的k时刻方程，统一写成矩阵形式*

​	先验：通过初始值和输入构建先验估计，利用运动方程得到：
$$
x_k=A_{k-1}x_{k-1}+v_k+w_k
$$
在**提升形式**下：我们有
$$
x=A(v+w)
$$
其中w是初始状态和运动噪声的提升形式：

![](img\22.png)

##### 提升形式推导：

​	我们注意：我们也要估计x0，且输入v0=x0已知，(x0先验(观测方程)=x0已知+wk噪声)

我们得到：
$$
x_0=\check{x_0}+w_0\\
x_1=A_0x_0+v_1+w_1\\
...
\\x_k=A_{k-1}x_{k-1}+v_k+w_k
$$
**由于每一项都有v+w的部分，因此，A矩阵的对角线上全为1。**其次，根据递推，上式的值代入下式总能得到规律，因此我们易得A矩阵的形式。



提升形式下，均值：（噪声w的均值为0）
$$
\check{x}=E[x]=E[A(v+w)]=Av
$$
协方差：
$$
\check{P}=E[(x-E[x])(x-E[x])^T]\\
=E[(A(v+w)-Av)(A(v+w)-Av)^T]
\\=E[Aww^TA^T](A为已知常数矩阵)
\\=AQA^T
$$
其中，(QK为每次噪声的方差)
$$
Q=E[ww^T]=diag(\check{P_0},Q_1,...,Q_K)
$$
**先验可以表示为：这么写的原因是运动方程，v的条件下x的值，p(x|v)**
$$
p(x|v)=N(\check{x},\check{P})=N(Av,AQA^T)
$$
​	继续，测量模型：
$$
y_k=C_kx_k+n_k
$$
我们要写成**y=Cx+n**的形式，接下来求C。
$$
C=diag(C_0,C_1,...,C_K)
$$
综上，我们有：

![](img\23.png)

其中 
$$
R=E[nn^T]=diag(R_0,R_1,...,R_k)
$$
**我们对上式进行推导：**

​	均值比较好理解，现在看一下协方差矩阵：
$$
p(y|v)=N(C\check x,\check P_y)\\
\check P_y=E[(y-E(y))(y-E(y))^T]\\
=E[(Cx+n-C\check x)(Cx+n-C\check x)^T]\\
=E[(CAw+n)(CAw+n)^T]\\
=(n_k之间相互独立，均值为零)E[CAww^TA^TC^T]+E[nn^T]\\
=C\check PC^T+R
$$

$$
E[(x-E(x))(y-E(y))]\\
=E[Aw(CAw+n)^T]\\
=E[Aww^TA^TC^T]+E[n^T]\\
=\check P C^T
$$

我们可以进行因式分解：
$$
p(x,y|v)(条件概率公式)\\
=\frac{p(x,y,v)}{p(v)}\\
=\frac{p(x,y,v)}{p(y,v)}\frac{p(y,v)}{p(v)}\\
=p(x|v,y)p(y|v)
$$
**我们只关心p(x|v,y)，这是全贝叶斯后验概率，观测y输入v的条件下，x的概率分布。**

我们用(2.2.3)节的方法，写成：(2.2.3推导)

![](img\24.png)

**附带上式推导：由2.2.3节中得到：**

![]()

利用式子(2.75)中的SMW恒等式，可以将上式转换为：

![](img\25.png)

**附带公式：**

![]()

我们为了便于优化，整理以下上式的均值项：

![](img\26.png)

我们代入：
$$
\check{x}=Av\\
\check P^{-1}=(AQA^T)^{-1}=A^{-T}Q^{-1}A^{-1}
$$
可以得到：

![](img\27.png)

这里需要计算A的逆，不过，A的逆有一个很好看的形式：？？？？？

![](img\28.png)

我们定义：
$$
z=[v \ \ \ y]^T\\
H=[A^{-1} \ \ \ C]^T\\
W=\begin{bmatrix}
Q&\\
&R
\end{bmatrix}
$$
我们就可以把系统写成：
$$
(H^TW^{-1}H)\hat{x}=H^TW^{-1}z
$$

### 3.1.4 Existence,Uniqueness and Observability

*译作：存在性，唯一性，能观性*

我们看线性高斯系统估计中有且仅有唯一解的情况：当且仅当H^TW^-1H可逆。有：
$$
\hat{x}=(H^TW^{-1}H)^{-1}H^TW^{-1}z\\
rank(H^TW^{-1}H)=N(K+1)
$$
由于W的逆为实对称正定矩阵，那么我们有：
$$
rank(H^TH)=rank(H^T)=N(K+1)
$$
**我们现在有两种情况：**

1. 我们有良好的初始先验 x0
2. 我们没有良好的初始先验

#### case1 Knowledge of initial state

我们直接看H^T即可：

![](img\29.png)

很明显，矩阵满秩，因此，我们能够得到唯一解xhat，只需要：
$$
\check P_0>0\\
Q_k>0(正定)
$$

#### case2 No knowledge of initial state

没有良好的初始先验值，我们就只能去掉H^T的第一列，变为：

![](img\30.png)

这个矩阵有K+1行，每块大小为N，将最上一行移到最下，得到：

![](img\31.png)

我们为了将其变为阶梯型，我们用A0T乘第一行，然后加到最后一行上；再用A0TA1T乘第二行，加到最后一行上，。。。。。。得到

![](img\32.png)

那么，我们只需要右下角的分块矩阵的秩为N即可，即：

![](img\33.png)

根据克莱哈密顿定理，我们直接给出结论：

**式(3.3.4)的解存在且唯一的条件是：**
$$
O=\begin{bmatrix}
C\\
CA\\
...\\
CA^{N-1}
\end{bmatrix}\\
Q_k>0,R_k>0,rank(O)=N
$$
我们用弹簧模型来解释一下：如图所示

![](img\34.png)

1. 当我们有初始状态时，系统总是能观的(无法移动任意一个(或多个)重物而不改变至少一个弹簧的长度)，这时候有一个唯一的最小能量状态。
2. 中间例子，就算不知道初始状态，也是能观的，有唯一解。
3. 最后一个就不行了，没有初始状态，也没有观测值，没有唯一解。

### 3.1.5 MAP的协方差

xhat代表x状态的最有估计，但是这个xhat有多少置信度？

## 3.2 Recursive Discrete-Time Smoothing

*译作：离散时间的递归平滑算法*

​	**概述：离散时间的批量估计中，给出了良好的结论，也容易从最小二乘的角度来理解？？？？？？思考**。但是，蛮力去解线性方程组是非常低效的。幸运的是，因为左侧的协方差矩阵的逆具有稀疏结构(对角块)，我们可以用这个性质有效地求解。
$$
(H^TW^{-1}H)\hat{x}=H^TW^{-1}z 
$$

### 3.2.1 Exploiting Sparsity in the Batch Solution

*译作：利用稀疏性求解批量估计*

​	如之前所讨论，我们有如下结论：

![](img\35.png)

其中，*代表非零块。可以采取稀疏**Cholesky分解**进行求解。Cholesky分解详见数值分析课程。

​	我们可以将上述矩阵分解为：
$$
H^TW^{-1}H=LL^T
$$
由于等式左侧为三对角块，因此L的形式如下：??????

![](img\36.png)

然后，先解：
$$
Ld=H^TW^{-1}z
$$
再解：
$$
L^T\hat x = d
$$


### 3.2.2 Cholesky Smoother

*译作：Cholesky 平滑算法*

​	我们看一下L的形式：

![](img\37.png)

​	我们展开并对比(这与矩阵的Cholesky分解过程较为类似)，一个位置一个位置的矩阵块对应相等即可，我们得到：

![](img\38.png)

如上所示，式子中我们标出了一些中间变量Ik。从这些式子可以看出，**我们先解L0(一步小型块稠密矩阵的分解)，然后将结果代入2式中得到L10，从上往下依次求解。**

​	接下来，我们从来解d：(方法与上类似)
$$
Ld=H^TW^{-1}z
$$
展开得到：

![](img\39.png)

​	最后，我们来解xhat：
$$
L^T \hat x = d
$$
展开块矩阵运算，得到：

![](img\40.png)

这样，我们就解完了。我们综合一下，整理得到：

![](img\41.png)

### 3.2.3 Rauch-Tung-Striebel 平滑算法

​	我们求解上式3.66c中的L(k,k-1)，这需要式子a，b的结果。解完后将结果代入3.66d中得到Ik：

![]()

令：
$$
\hat P_{k,f}=I_k^{-1}\\
\check P_{k,f}=(A_{k-1}I_{k-1}^{-1}+Q_k)^{-1}
$$
我们将上式写成：
$$
\check P_{k,f}=(A_{k-1}I_{k-1}^{-1}+Q_k)^{-1}\\
\hat P_{k,f}=\check P_{k,f}^{-1}+C_k^TR_k^{-1}C_k\\
$$
**这里，第一个式子表示预测的协方差，通过输入方程。而第二个式子表示修正的协方差，通过观测方程。**

​	我们用卡尔曼增益Kk来表示：定义
$$
K_k=\hat P_{k,f}C_k^TR_k^{-1}\\
经过推导：（P54）\\
\hat P_{k,f}=(1-K_kC_k)\check P_{k,f}
$$
​	







![](img\42.png)

**前向的五个过程被称为卡尔曼滤波器，这六个公式表达的式RTS平滑算法。**这件事情成立的原因，正式由于优化算法左侧矩阵的特殊三对角块的稀疏结构。

## 3.3 离散时间的递归滤波算法

*本章其余部分，介绍其他几种推导和理解卡尔曼滤波器的方法。*

这张图体现了批量估计算法和滤波器估计(卡尔曼滤波)的区别。

![](img\43.png)

### 3.3.1 Factoring the Batch Solution

​	为了把结论写成递归的形式，我们重新排列一下批量优化中的变量，重新定义z、H、W(只是交换了矩阵行的位置)。

![](img\44.png)

如上式，这些线分开了不同时间戳的变量。

​	现在，我们来考虑其在概率密度函数层面的分解(factorization)。我们现在想考虑k时刻的状态，**我们进行边缘化操作，把其他时刻的变量积分出去**
$$
p(x_k|v,y)=\int_{x_i,\forall i≠k}p(x_i|v,y)dx_{i,\forall i≠k}
$$
​	为了实现分解，我们要用到H的稀疏结构。先把H分解成12个块(仅有6个非零块)。如下所示：

![](img\45.png)

举个k=2 ， K=4的例子：

![](img\46.png)

我们把z和W也转换成这个样式：

![](img\47.png)

那么，具体的矩阵H^TW^-1H计算为：

![]()

其中，Lij为中间变量。

同样，我们计算H^TW^{-1}z计算为：

![]()

其中，ri为中间变量。

### 3.3.2 通过MAP推导卡尔曼滤波

​	讲解如何将上一节的前向估计转换为递归滤波器(卡尔曼滤波器的形式)。假设我们现在已经有了k-1时刻的前向估计：
$$
\{\hat x_{k-1},\hat P_{k-1}\}
$$
我们要计算：
$$
\{\hat x_k,\hat P_k\}
$$
其实，我们无需从头开始，只需要使用k-1时刻的状态以及k时刻的vk(输入)，yk(观测)来估计k时刻的状态：
$$
\{\hat x_{k-1},\hat P_{k-1},v_k,y_k\}->\{\hat x_k,\hat P_k\}
$$

因此，我们直接写出k-1时刻的估计与k时刻估计的关系：

![](img\52.png)

我们定义：
$$
\hat x = 
\begin{bmatrix}
\hat x'_{k-1}\\
\hat x_k
\end{bmatrix}
$$
其中，x'k-1是指利用了vk和yk估计得到的(利用了未来信息)。

### 3.3.3 通过贝叶斯推断推导卡尔曼滤波

设k-1时刻的高斯先验为：
$$
p(x_{k-1}|\check x_0,v_{1:k-1},y_{0,k-1})=N(\hat x_{k-1},\hat P_{k-1})
$$
首先，**对于预测部分**，我们考虑最近时刻的输入vk，来计算k时刻的"先验"：
$$
p(x_k|\check x_0,v_{1:k},y_{0:k-1})=N(\check x_k,\check P_k)
$$
其中：
$$
\check P_k = A_{k-1}\hat P_{k-1}A^T_{k-1}+Q_k\\
\check x_k = A_{k-1}\hat x_{x-1}+v_k
$$
**我们简单对上述两式进行一下推导：**
$$
\check P_k(根据方差定义)=E[(x_k-E[x_k])(x_k-E[x_k])^T]
\\=E[(A_{k-1}x_{k-1}+v_k+w_k-A_{k-1}\hat x_{k-1}-v_k)(A_{k-1}x_{k-1}+v_k+w_k-A_{k-1}\hat x_{k-1}-v_k)^T]
\\=A_{k-1}E[(x_{k-1}-\hat x_{k-1})(x_{k-1}-\hat x_{k-1})^T]A^T_{k-1}+E[w_kw_k^T]+E[w_k×X]
\\(矩阵X为由xk分量组成的随机变量矩阵，与w_k不相关)
\\=......+E[w_k]E[X]=......+0\\
=A_{k-1}\hat P_{k-1}A^T_{k-1}+Q_k
$$
**之后，对于更新部分(测量)，**我们将状态与最近的依次测量写成联合高斯分布的形式：

![](img\48.png)

根据2.2.3节，我们总能将p(x,y)写成p(x|y)p(y)

其中：

![](img\49.png)

根据上述推断，我们直接得到xk的条件分布，即后验概率(上式把yk拿到了后面)：

![](img\50.png)

此处，我们定义xk_hat 为均值，Pk_hat为方差，代入之前的结果则有：

![](img\51.png)

### 3.3.4 从增益最优化的角度来看卡尔曼滤波

​	我们通常说**卡尔曼滤波是最优的**。同时，我们也确实从MAP中推导出了卡尔曼滤波与递归形式优化之间的关系。现从增益最优化的角度来看卡尔曼滤波。

假设一个估计器：
$$
\hat x_k=\check x_k+K_k(y_k-C_k\check x_k)
$$
但此时**不知道如何选取K的值**，我们定义状态误差为：
$$
\hat e_k= \hat x_k-x_k
$$
则有：
$$
E[\hat e_k \hat e_k^T]=(1-K_kC_k)\check P_k(1-K_kC_k)^T+K_kR_kK_k^T
$$
解释：协方差估计，第二项是，测量Y噪声带来的协方差。

我们定义出一个**代价函数**：
$$
J(K_k)=\frac{1}{2} trE[\hat e_k \hat e_k^T]=E[\frac{1}{2}\hat e_k^T \hat e_k]
$$
使这个误差最小化，求Kk即可。

因此，对K求导得到：
$$
前置公式：\frac{\part trXY}{\part X}=Y^T,\frac{\part trXZX^T}{\part X}=2XZ
\\ \frac{\part J(K_k)}{\part K_k}=-(1-K_kC_k)\check P_kC_k^T+K_kR_k
$$
令偏导数=0即可求得：
$$
K_k=\check P_k C_k^T(C_k \check P_k C_k^T+R_k)^{-1}
$$
这就是卡尔曼增益的通常表达式。

### 3.3.5 关于卡尔曼滤波的讨论

以下是卡尔曼滤波的要点：

1. 对于高斯噪声的线性系统，卡尔曼滤波是**最优线性无偏估计**。这意味着它给出的解的协方差位于克拉美罗下界处。
2. 必须有初始状态 x0 P0
3. 协方差部分与均值部分可以独立地递推，有时可以计算一个固定的Kk，用于所有时刻的均值修正。这种做法称为**固定状态的卡尔曼滤波**。
4. 实现当中我们用实际的传感器读数，yk,meas
5. 从后向过程一样可以推导处类似的滤波方法，这种方法会沿着时间反向传递。

非线性场合下，我们需要使用——**扩展卡尔曼滤波器EKF**。

### 3.3.6 误差动态过程

### 3.3.7 存在性，唯一性以及能观性







## 3.4 连续时间的批量估计问题

### 3.4.1 高斯过程回归

### 3.4.2 一种稀疏的高斯过程先验

### 3.4.3 线性时不变情况

### 3.4.4 与批量离散时间情况的关系