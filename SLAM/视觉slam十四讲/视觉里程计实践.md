# 视觉里程计实践

实践代码在slambook/project中。

## 9.1 确定程序框架

#### 9.1.1 基本程序框架

​	Linux程序的组织方式：在编写一个小规模库时，我们通常会建立一些文件夹，把**源代码、头文件、文档、测试数据、配置文件、日志等等**分类存放。（例如：OpenCV有core、imgproc、features2d等模块）。

​	我们通常建立这些文件夹来组织代码文件：

1. bin用来存放可执行的二进制
2. include/myslam存放slam模块的头文件，主要是.h文件。目的：当把包含目录设到include时，在引用自己的头文件时，需要写include"myslam/xxx.h"，这样不容易和别的库混淆。
3. src存放源代码文件，主要是cpp
4. test存放测试用的文件，也是cpp
5. lib存放编译好的库文件
6. config存放配置文件
7. cmake_modules第三方库的cmake文件，在使用g2o之类的库中会用到。

#### 9.1.2 确定基本数据结构

​	为了让程序正常运行，我们要设计好数据单元，以及程序处理的流程。一个SLAM程序中，我们抽象出几个基本概念：

1. **帧**：一个帧是相机采集到的图像单位。它主要包含一个图像，此外还有特征点、位姿、内参等信息。       在视觉SLAM中，我们会谈论关键点(Key-frame)。由于相机采集的数据很多，存储所有的数据并不现实。**通常做法：把某些我们认为更重要的帧保存起来，认为相机轨迹可以用这些关键帧来描述。**
2. **路标**：路标点即图像中的特征点，当相机运动之后，我们还能估计它们的3D位置。通常，会把路标点放在一个地图中，并将新来的帧与地图中的路标点进行匹配，估计相机位姿。

除此之外，我们需要一些工具，让程序写起来更流畅。例如：

1. **配置文件**：在写程序中，遇到的参数(如PID参数、相机内参、特征点数量、匹配时萱额比例等等)。我们一般把这些参数放在外部的一个配置文件中。程序运行时读取该配置文件中的参数值。
2. **坐标变换**：我们会经常需要在坐标系之间进行坐标变换。例如：世界坐标系到相机坐标系、相机坐标系到归一化相机坐标系。定义一个类把这些操作都放在一起会比较方便。

**因此，下面我们就来定义帧、路标这几个概念，在C++中都以类来表示**。我们尽量保证一个类有单独的头文件和源文件，避免把许多个类放在同一个文件中。**其次，将函数生命放在头文件，实现放在源文件中**。我们的程序是偏向算法而非软件工程的，因此**不重点考虑复杂的类继承关系、接口等问题，尽量考虑算法的正确实现，以及是否便于扩展**。编写算法时，**我们应将复杂算法分解成若干步骤，方便调试以及修改**。

#### 9.1.3 程序模块简介

​	现在，我们编写第一版的视觉里程计VO，定为版本0.1。我们一共写5个类：Frame为帧，Camera为相机模型，MapPoint为特征点/路标点，Map管理特征点，Config提供配置参数。它们之间的关系如下图所示：

![]()

#### 9.1.4 Camera类

​	Camera类存储相机的内参和外参，并完成相机坐标系、像素坐标系、和世界坐标系之间的坐标变换。在世界坐标系中需要一个相机的(变动的)外参，我们以参数的形式传入。

```C++
#ifndef CAMERA_H
#define CAMERA_H
#include "myslam/common_include.h"

//define Camera class
namespace myslam
{
//define Pinhole RGB-D camera model
class Camera:
{
public:
	typedef std::shared_ptr<Camera> Ptr;
    float fx_,fy_,cx_,cy_,depth_scale_; //Camera intrinsics
    
    Camera(); //default create function
    Camera(float fx,float fy,float cx,float cy,float depth_scale=0):
    fx_(fx),fy_(fy),cx_(cx),cy_(cy),depth_scale_(depth_scale){}
    
    //coordinate transform : world , camera , pixel
    Vector3d world2camera(const Vector3d& p_w , const SE3& T_c_w);
    Vector3d camera2world(const Vector3d& p_c, const SE3& T_c_w);
    Vector2d camera2pixel(const Vector3d& p_c);
    Vector3d pixel2camera(const Vector2d& p_p, double depth);
    Vector3d pixel2world( const Vector3d& p_w, const SE3& T_c_w);
    Vector2d world2pixel(const Vector2d& p_p, const SE3& T_c_w, double depth);
}
    
}


#endif  // CAMERA_H
```

注解：（由上往下）

1. 防止头文件重复引用的ifndef宏定义。每个头文件程序都必须有这样的宏定义。

2. 命名空间：namespace myslam

3. common_include.h文件中包含了常用的头文件。

4. 智能指针：typedef std::shared_ptr<Camera> Ptr ，指向Camera类对象的指针。Camera::Ptr 即可。

5. 我们用Sophus::SE3 来表达相机的位姿。

   

给出Camera方法的实现：

```c++
#include "myslam/camera.h"
namespace myslam
{
Vector3d Camera::world2camera(const Vector3d& p_w , const SE3& T_c_w)
{
    //Pc=TcwPw
    return T_c_w*p_w;
}
Vector3d Camera::camrea2world(const Vector3d& p_c , const SE3& T_c_w)
{
    //
    return T_c_w.inverse()*p_c;
}
    
Vector2d camera2pixel(const Vector3d& p_c)
{
    //can be changed
    return Vector2d(fx_*p_c(0,0)/p_c(2,0)+cx_,fy_*p_c(1,0)/p_c(2,0)+cy_);
}

Vector3d Camera::pixel2camera ( const Vector2d& p_p, double depth )
{
    return Vector3d (
        ( p_p ( 0,0 )-cx_ ) *depth/fx_,
        ( p_p ( 1,0 )-cy_ ) *depth/fy_,
        depth
    );
}
Vector2d Camera::world2pixel ( const Vector3d& p_w, const SE3& T_c_w )
{
    return camera2pixel ( world2camera ( p_w, T_c_w ) );
}
Vector3d Camera::pixel2world ( const Vector2d& p_p, const SE3& T_c_w, double depth )
{
    return camera2world ( pixel2camera ( p_p, depth ), T_c_w );
}
    
}
```

#### 9.1.4 Frame 类

​	由于Frame类是基本数据单元，在许多地方会用到，初始的Frame类提供基本的数据存储和接口。这个类需要良好的拓展性。

​	头文件Frame.h

```c++
#ifndef FRAME_H
#define FRAME_H

#include "myslam/common_include.h"
#include "myslam/camera.h"


class FRAME
{
public:
    typedef std::shared<Frame> Ptr;
    unsigned long id_; //id of this frame
    double time_step_;//when it is recorded
    SE3 T_c_w_; // transform from world to camera
    Camera::Ptr camera_;//Pinhole RGB-D Camera model
    Mat color_ , depth_; //basic image from camera
    
public:
    Frame();
    Frame(unsigned long id,double time_step=0,SE3 T_c_w=SE3(),Camera::Ptr camera=nullptr , Mat color=Mat(),Mat depth=Mat());
    ~Frame();
    
    //factory function
    static Frame::Ptr createFrame();
    
    //find the depth in depth map
    double findDepth(const cv::KeyPoint& kp);
    
    //check if a point is in this frame
    bool isInFrame(const Vector3d& pt_world);
    
};

#endif  // FRAME_H
```

注释：在Frame中，我们定义了ID、时间戳、位姿、相机、图像这几个量，这是一帧当中含有的最重要的信息。在方法中，我们提取了几个重要方法：创建Frame、寻找给定点的深度、获取相机光心、判断某个点是否在视野内等等。

​	Frame.cpp 实现

```c++
#include "myslam/frame.h"

namespace myslam
{

Frame::Frame ( long id, double time_stamp, SE3 T_c_w, Camera::Ptr camera, Mat color, Mat depth )
: id_(id), time_stamp_(time_stamp), T_c_w_(T_c_w), camera_(camera), color_(color), depth_(depth)
{

}
	
Frame::Frame()
: id_(-1), time_stamp_(-1), camera_(nullptr)
{

}

Frame::~Frame()
{
	//
}

Frame::Ptr Frame::createFrame()
{
    static long factory_id = 0;
    return Frame::Ptr( new Frame(factory_id++) );
}
  
double Frame::findDepth ( const cv::KeyPoint& kp )
{
    int x = cvRound(kp.pt.x);
    int y = cvRound(kp.pt.y);
    ushort d = depth_.ptr<ushort>(y)[x]; // can be changed
    if ( d!=0 )
    {
        return double(d)/camera_->depth_scale_;
    }
    else 
    {
        // check the nearby points 
        int dx[4] = {-1,0,1,0};
        int dy[4] = {0,-1,0,1};
        for ( int i=0; i<4; i++ )
        {
            d = depth_.ptr<ushort>( y+dy[i] )[x+dx[i]];
            if ( d!=0 )
            {
                return double(d)/camera_->depth_scale_;
            }
        }
    }
    return -1.0;
}
    
Vector3d Frame::getCamCenter() const
{
    return T_c_w_.inverse().translation();
}

bool Frame::isInFrame ( const Vector3d& pt_world )
{
    //transform from p_world to pixel , and judge this pixel is leagel or not
    Vector3d p_cam = camera_->world2camera( pt_world, T_c_w_ );
    if ( p_cam(2,0)<0 ) 
        return false;
    Vector2d pixel = camera_->world2pixel( pt_world, T_c_w_ );
    
    //is this point over scale
    return pixel(0,0)>0 && pixel(1,0)>0 
        && pixel(0,0)<color_.cols 
        && pixel(1,0)<color_.rows;
}

}
```

#### 9.1.5 MapPoint 类

​	MapPoint表示路标点。我们将估计它的世界坐标，并且我们会拿当前帧提取到的特征点与地图中的路标点匹配，来估计相机的运动，因此还需要存储它对应的描述子。此外我们会记录一个点被观测到的次数和被匹配到的次数，作为评价它的好坏程度的指标。

```c++
#ifndef MAPPOINT_H
#define MAPPOINT_H

namespace myslam
{
    
class Frame;
class MapPoint
{
public:
    typedef shared_ptr<MapPoint> Ptr;
    unsigned long      id_; // ID
    Vector3d    pos_;       // Position in world
    Vector3d    norm_;      // Normal of viewing direction 
    Mat         descriptor_; // Descriptor for matching 
    int         observed_times_;    // being observed by feature matching algo.
    int         correct_times_;     // being an inliner in pose estimation
    
    MapPoint();
    MapPoint( long id, Vector3d position, Vector3d norm );
    
    // factory function
    static MapPoint::Ptr createMapPoint();
};
}

#endif // MAPPOINT_H
```

mappoint.cpp实现

```c++
#include "myslam/common_include.h"
#include "myslam/mappoint.h"

namespace myslam
{

MapPoint::MapPoint()
: id_(-1), pos_(Vector3d(0,0,0)), norm_(Vector3d(0,0,0)), observed_times_(0), correct_times_(0)
{

}

MapPoint::MapPoint ( long id, Vector3d position, Vector3d norm )
: id_(id), pos_(position), norm_(norm), observed_times_(0), correct_times_(0)
{

}

MapPoint::Ptr MapPoint::createMapPoint()
{
    static long factory_id = 0;
    return MapPoint::Ptr( 
        new MapPoint( factory_id++, Vector3d(0,0,0), Vector3d(0,0,0) )
    );
}

}
```

#### 9.1.6 Map 类

​	Map类管理着所有的路标点，并且负责添加新路标、删除不好的路标等工作。VO的匹配过程只需要和Map打交道即可。当然Map也会有很多操作，但现在只定义主要的数据结构。

```c++
//只给出Map类的定义
class Map
{
public:
    typedef std::shared_ptr<Map> Ptr;
    unordered_map<unsigned long,MapPoint::Ptr> map_points_;//all landmarks
    unordered_map<unsigned long, Frame::Ptr>keyframes_; // all key-frames
    
    Map(){};
    
    void insertKeyFrame(Frame::Ptr frame);
    void insertMapPoint(MapPoint::Ptr map_point);
};
```

Map类中实际存储了各个关键帧和路标点，既需要随即访问，又需要随时插入和删除，因此我们使用散列(Hash)来存储它们。



**unordered_map**

#### 9.1.7 Config 类

​	Config类负责参数文件的读取，并在程序任意地方都可以随时提供参数的值。所以我们把Config写成单件模式(Singleton)。它只有一个全局对象，当我们设置参数文件时，创建该对象并读取参数文件，随后就可以在任意地方访问参数值，最后在程序结束时自动销毁。

```c++
#ifndef CONFIG_H
#define CONFIG_H

#include "myslam/common_include.h" 

namespace myslam 
{
class Config
{
private:
    static std::shared_ptr<Config> config_; 
    cv::FileStorage file_;
    
    Config () {} // private constructor makes a singleton
public:
    ~Config();  // close the file when deconstructing 
    
    // set a new config file 
    static void setParameterFile( const std::string& filename ); 
    
    // access the parameter values
    template< typename T >
    static T get( const std::string& key )
    {
        return T( Config::config_->file_[key] );
    }
};
}

#endif // CONFIG_H
```

注解：

1. **我们把构造函数声明为私有，防止这个类的对象在别处建立，其只能在setParameterFile时构造。**实际构造的是Config智能指针：static std::shared_ptr<Config> config_。
2. 在读取文件方面，我们使用OpenCV提供的FileStorage类。它可以读取一个YAML文件，且可以访问其中任意一个字段。由于参数实质值可能为整数、浮点数或字符串，所以我们通过一个模板函数get，来获得任意类型的参数值。

config.cpp的实现

```c++
#include "myslam/config.h"

namespace myslam 
{
    
void Config::setParameterFile( const std::string& filename )
{
    if ( config_ == nullptr )
        config_ = shared_ptr<Config>(new Config);
    config_->file_ = cv::FileStorage( filename.c_str(), cv::FileStorage::READ );
    if ( config_->file_.isOpened() == false )
    {
        std::cerr<<"parameter file "<<filename<<" does not exist."<<std::endl;
        config_->file_.release();
        return;
    }
}

Config::~Config()
{
    if ( file_.isOpened() )
        file_.release();
}

shared_ptr<Config> Config::config_ = nullptr;

}
```

实现方法：**我们只要判断以下参数文件是否存在，之后我们就可以获得了全局对象，则可以在任何地方获取参数文件里的参数。**

​	例如：当我想要定义相机的焦距fx时，按照以下几个操作步骤即可：

1. 在参数文件中加入："Camera.fx:500"
2. 在代码中使用：

```c++
myslam::Config::setParameterFile("parameter.yaml");
double fx = myslam::Config::get<double>("Camera.fx");
```

这样即可获得fx的值。

## 9.2 基本的VO：特征提取和匹配

​	下面实现基本的VO，根据特征点法：根据输入图像，匹配特征点，并且计算相机运动和特征点位置。前面讨论都是两两帧之间的位姿估计，**但实际中，仅凭两帧是不够的，我们会把特征点缓存成一个小地图，计算当前帧与地图之间的位置关系。**不过先实现基础目标：两两帧。

#### 9.2.1 两两帧视觉里程计

​	只关心两帧之间的运动估计，并且不优化特征点的位置，这样做也可以得到一条运动轨迹，但是效果不行。

过程大致如下所示：

1. 对新来的当前帧，提取关键点和描述子（为了进行特征点匹配，详细见第七章）
2. 若系统未初始化，以该帧为参考帧，根据深度图计算关键点的3D位置，返回True
3. 估计参考帧与当前帧之间的运动
4. 判断上述估计是否成功(特征点少可能失败)
5. 若成功，则把当前帧作为新的参考帧，返回True
6. 若失败，计算连续丢失帧数。当连续丢失超过一定帧数，设置VO状态为丢失，算法结束，返回False；若未超过，则返回True

visual_odometry.h头文件

```c++
#ifndef VISUALODOMETRY_H
#define VISUALODOMETRY_H

#include "myslam/common_include.h"
#include "myslam/map.h"

#include <opencv2/features2d/features2d.hpp>

namespace myslam 
{
class VisualOdometry
{
public:
    typedef shared_ptr<VisualOdometry> Ptr;
    enum VOState {
        INITIALIZING=-1,
        OK=0,
        LOST
    };
    
    VOState     state_;     // current VO status
    Map::Ptr    map_;       // map with all frames and map points
    Frame::Ptr  ref_;       // reference frame 
    Frame::Ptr  curr_;      // current frame 
    
    cv::Ptr<cv::ORB> orb_;  // orb detector and computer 
    vector<cv::Point3f>     pts_3d_ref_;        // 3d points in reference frame 
    vector<cv::KeyPoint>    keypoints_curr_;    // keypoints in current frame
    Mat                     descriptors_curr_;  // descriptor in current frame 
    Mat                     descriptors_ref_;   // descriptor in reference frame 
    vector<cv::DMatch>      feature_matches_;
    
    SE3 T_c_r_estimated_;  // the estimated pose of current frame 
    int num_inliers_;        // number of inlier features in icp
    int num_lost_;           // number of lost times
    
    // parameters 
    int num_of_features_;   // number of features
    double scale_factor_;   // scale in image pyramid
    int level_pyramid_;     // number of pyramid levels
    float match_ratio_;      // ratio for selecting  good matches
    int max_num_lost_;      // max number of continuous lost times
    int min_inliers_;       // minimum inliers
    
    double key_frame_min_rot;   // minimal rotation of two key-frames
    double key_frame_min_trans; // minimal translation of two key-frames
    
public: // functions 
    VisualOdometry();
    ~VisualOdometry();
    
    bool addFrame( Frame::Ptr frame );      // add a new frame 
    
protected:  
    // inner operation 
    void extractKeyPoints();
    void computeDescriptors(); 
    void featureMatching();
    void poseEstimationPnP(); 
    void setRef3DPoints();
    
    void addKeyFrame();
    bool checkEstimatedPose(); 
    bool checkKeyFrame();
    
};
}

#endif // VISUALODOMETRY_H
```

解释：

1. 我们定义一个VO类，声明一个VO类的对象后，就代表我们获得了一套VO算法(包括基本参数：状态、最小特征点数量等等，基本方法(函数)：特征点匹配函数、关键点提取函数等等)

2. VO本身有若干种状态：设定第一帧、顺利跟踪、丢失等等。在本例，考虑最简单的三个状态：初始化、正常、丢失

3. 我们把一些中间变量定义在类中，方便访问。

4. 特征提取和匹配当中的参数，从参数文件中提取。例如：

   ```c++
   VisualOdometry::VisualOdometry() :
   state_ ( INITIALIZING ), ref_ ( nullptr ), curr_ ( nullptr ), map_ ( new Map ), num_lost_ ( 0
   ), num_inliers_ ( 0 )
   {
       //初始化列表加参数读取
       num_of_features_ = Config::get<int> ( "number_of_features" );
       scale_factor_ =Config::get<double> ( "scale_factor" );
       level_pyramid_ = Config::get<int> ( "level_pyramid" );
       match_ratio_ =Config::get<float> ( "match_ratio" );
   }
   ```

5.addFrame函数是外部调用的接口，使用VO时，将图像装入Frame后，调用addFrame估计其位姿，该函数根据VO所处的状态实现不同的操作。

```c++
bool VisualOdometry::addFrame ( Frame::Ptr frame )
{
    switch ( state_ )
    {
        case INITIALIZING:
        {
            state_ = OK;
            curr_ = ref_ = frame;
            map_->insertKeyFrame ( frame );
            // extract features from first frame 
            extractKeyPoints();
            computeDescriptors();
            // compute the 3d position of features in ref frame 
            setRef3DPoints();
            break;
        }
        case OK:
        {
            curr_ = frame;
            extractKeyPoints();
            computeDescriptors();
            featureMatching();
            poseEstimationPnP();
            if ( checkEstimatedPose() == true ) // a good estimation
            {
                curr_->T_c_w_ = T_c_r_estimated_ * ref_->T_c_w_;  // T_c_w = T_c_r*T_r_w 
                ref_ = curr_;
                setRef3DPoints();
                num_lost_ = 0;
                if ( checkKeyFrame() == true ) // is a key-frame
                {
                    addKeyFrame();
                }
            }
            else // bad estimation due to various reasons
            {
                num_lost_++;
                if ( num_lost_ > max_num_lost_ )
                {
                    state_ = LOST;
                }
                return false;
            }
            break;
        }
        case LOST:
        {
            cout<<"vo has lost."<<endl;
            break;
        }
    }

    return true;
}
```



同时，我们加入测试函数 run_vo.cpp

```c++
// -------------- test the visual odometry -------------
#include <fstream>
#include <boost/timer.hpp>
#include <opencv2/imgcodecs.hpp>
#include <opencv2/highgui/highgui.hpp>
#include <opencv2/viz.hpp> 

#include "myslam/config.h"
#include "myslam/visual_odometry.h"

int main ( int argc, char** argv )
{
    if ( argc != 2 )
    {
        cout<<"usage: run_vo parameter_file"<<endl;
        return 1;
    }

    myslam::Config::setParameterFile ( argv[1] );//argv第一个参数：参数文件路径，并且设置参数。
    myslam::VisualOdometry::Ptr vo ( new myslam::VisualOdometry );//一个视觉里程计

    string dataset_dir = myslam::Config::get<string> ( "dataset_dir" );//数据文件路径
    cout<<"dataset: "<<dataset_dir<<endl;
    ifstream fin ( dataset_dir+"/associate.txt" );
    if ( !fin )
    {
        cout<<"please generate the associate file called associate.txt!"<<endl;
        return 1;
    }
	
    //从数据文件中读取图像以及参数
    vector<string> rgb_files, depth_files;
    vector<double> rgb_times, depth_times;
    while ( !fin.eof() )
    {
        string rgb_time, rgb_file, depth_time, depth_file;
        fin>>rgb_time>>rgb_file>>depth_time>>depth_file;
        rgb_times.push_back ( atof ( rgb_time.c_str() ) );
        depth_times.push_back ( atof ( depth_time.c_str() ) );
        rgb_files.push_back ( dataset_dir+"/"+rgb_file );
        depth_files.push_back ( dataset_dir+"/"+depth_file );

        if ( fin.good() == false )
            break;
    }
	
    
    myslam::Camera::Ptr camera ( new myslam::Camera );//声明一个相机模型类的指针
    
    // visualization
    cv::viz::Viz3d vis("Visual Odometry");
    cv::viz::WCoordinateSystem world_coor(1.0), camera_coor(0.5);
    cv::Point3d cam_pos( 0, -1.0, -1.0 ), cam_focal_point(0,0,0), cam_y_dir(0,1,0);
    cv::Affine3d cam_pose = cv::viz::makeCameraPose( cam_pos, cam_focal_point, cam_y_dir );
    vis.setViewerPose( cam_pose );
    
    world_coor.setRenderingProperty(cv::viz::LINE_WIDTH, 2.0);
    camera_coor.setRenderingProperty(cv::viz::LINE_WIDTH, 1.0);
    vis.showWidget( "World", world_coor );
    vis.showWidget( "Camera", camera_coor );

    cout<<"read total "<<rgb_files.size() <<" entries"<<endl;
    for ( int i=0; i<rgb_files.size(); i++ )
    {
        Mat color = cv::imread ( rgb_files[i] );
        Mat depth = cv::imread ( depth_files[i], -1 );
        if ( color.data==nullptr || depth.data==nullptr )
            break;
        myslam::Frame::Ptr pFrame = myslam::Frame::createFrame();
        pFrame->camera_ = camera;
        pFrame->color_ = color;
        pFrame->depth_ = depth;
        pFrame->time_stamp_ = rgb_times[i];

        boost::timer timer;
        vo->addFrame ( pFrame );//进行特征点匹配以及相机位姿估计
        cout<<"VO costs time: "<<timer.elapsed()<<endl;
        
        if ( vo->state_ == myslam::VisualOdometry::LOST )
            break;
        SE3 Tcw = pFrame->T_c_w_.inverse();
        
        // show the map and the camera pose 
        cv::Affine3d M(
            cv::Affine3d::Mat3( 
                Tcw.rotation_matrix()(0,0), Tcw.rotation_matrix()(0,1), Tcw.rotation_matrix()(0,2),
                Tcw.rotation_matrix()(1,0), Tcw.rotation_matrix()(1,1), Tcw.rotation_matrix()(1,2),
                Tcw.rotation_matrix()(2,0), Tcw.rotation_matrix()(2,1), Tcw.rotation_matrix()(2,2)
            ), 
            cv::Affine3d::Vec3(
                Tcw.translation()(0,0), Tcw.translation()(1,0), Tcw.translation()(2,0)
            )
        );
        
        cv::imshow("image", color );
        cv::waitKey(1);
        vis.setWidgetPose( "Camera", M);
        vis.spinOnce(1, false);
    }

    return 0;
}

```

## 9.3 改进：优化PnP

​	接着，我们继续尝试改进一些VO的算法。我们来尝试RANSAC PnP加上迭代优化的方式估计相机位姿。

​	本节的目标是估计位姿而非结构，我们以相机位姿ξ为优化变量，通过最小化重投影误差，来构建优化问题。

## 9.4 改进：局部地图

​	本节，我们将VO匹配到的特征点放到地图中，并将当前帧与地图点进行匹配，计算位姿。与之前做法差异如下：

![](/home/gzh/图片/2021-11-03 15-37-30 的屏幕截图.png)

​	之前两两帧间比较时，我们只计算参考帧与当前帧之间的特征匹配和运动关系，在计算之后把当前帧设为新的参考帧**。而在使用地图的VO中，每个帧为地图贡献一些信息，比如：添加新的特征点或更新旧的特征点的位置估计。**地图中的特征点位置往往使用**世界坐标系表示**。因此，当前帧到来时，我们求它和地图之间的特征匹配与运动关系，即直接计算了Tcw。

​	一般来说，地图分为**局部（Local）和全局（Global）两种**。局部地图描述了附近的特征点信息——我们只保留距离相机当前位置较近的特征点，而把远的或视野以外的特征点丢掉。这些特征点是用来和当前帧匹配来求相机位置的，希望速度较快。另一方面，全局地图记录了从SLAM运行以来的所有特征点，它规模较大，用来表达整个环境，也主要用于回环检测。

